{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f95a8a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/pavan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Imports & basic config\n",
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Model & training config\n",
    "MODEL_NAME = \"t5-small\"  # can later try \"google/t5-base\" in bigger GPU\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "MAX_TARGET_LENGTH = 256\n",
    "\n",
    "OUTPUT_DIR = \"models/t5-billsum\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b9bed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum = load_dataset(\"billsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49e62fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SECTION 1. LIABILITY OF BUSINESS ENTITIES PROV...</td>\n",
       "      <td>Shields a business entity from civil liability...</td>\n",
       "      <td>A bill to limit the civil liability of busines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Human Rights Information Act - Requires certai...</td>\n",
       "      <td>Human Rights Information Act</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Jackie Robinson Commemorative Coin Act - Direc...</td>\n",
       "      <td>Jackie Robinson Commemorative Coin Act</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SECTION 1. NONRECOGNITION OF GAIN WHERE ROLLOV...</td>\n",
       "      <td>Amends the Internal Revenue Code to provide (t...</td>\n",
       "      <td>To amend the Internal Revenue Code to provide ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Native American Energy Act - (Sec. 3) Amends t...</td>\n",
       "      <td>Native American Energy Act</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  SECTION 1. LIABILITY OF BUSINESS ENTITIES PROV...   \n",
       "1  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "2  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "3  SECTION 1. NONRECOGNITION OF GAIN WHERE ROLLOV...   \n",
       "4  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Shields a business entity from civil liability...   \n",
       "1  Human Rights Information Act - Requires certai...   \n",
       "2  Jackie Robinson Commemorative Coin Act - Direc...   \n",
       "3  Amends the Internal Revenue Code to provide (t...   \n",
       "4  Native American Energy Act - (Sec. 3) Amends t...   \n",
       "\n",
       "                                               title  \n",
       "0  A bill to limit the civil liability of busines...  \n",
       "1                       Human Rights Information Act  \n",
       "2             Jackie Robinson Commemorative Coin Act  \n",
       "3  To amend the Internal Revenue Code to provide ...  \n",
       "4                         Native American Energy Act  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "billsum = load_dataset(\"billsum\")\n",
    "\n",
    "# Convert each split into Pandas DataFrame\n",
    "train_df = billsum[\"train\"].to_pandas()\n",
    "test_df = billsum[\"test\"].to_pandas()\n",
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e3621c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SECTION 1. ENVIRONMENTAL INFRASTRUCTURE.\\n\\n  ...</td>\n",
       "      <td>Amends the Water Resources Development Act of ...</td>\n",
       "      <td>To make technical corrections to the Water Res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>That this Act may be cited as the ``Federal Fo...</td>\n",
       "      <td>Federal Forage Fee Act of 1993 - Subjects graz...</td>\n",
       "      <td>Federal Forage Fee Act of 1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>.  Merchant Marine of World War II Congression...</td>\n",
       "      <td>Merchant Marine of World War II Congressional ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Small Business Modernization Act of 2004 - Ame...</td>\n",
       "      <td>To amend the Internal Revenue Code of 1986 to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SECTION 1. SHORT TITLE.\\n\\n    This Act may be...</td>\n",
       "      <td>Fair Access to Investment Research Act of 2016...</td>\n",
       "      <td>Fair Access to Investment Research Act of 2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  SECTION 1. ENVIRONMENTAL INFRASTRUCTURE.\\n\\n  ...   \n",
       "1  That this Act may be cited as the ``Federal Fo...   \n",
       "2  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "3  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "4  SECTION 1. SHORT TITLE.\\n\\n    This Act may be...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Amends the Water Resources Development Act of ...   \n",
       "1  Federal Forage Fee Act of 1993 - Subjects graz...   \n",
       "2  .  Merchant Marine of World War II Congression...   \n",
       "3  Small Business Modernization Act of 2004 - Ame...   \n",
       "4  Fair Access to Investment Research Act of 2016...   \n",
       "\n",
       "                                               title  \n",
       "0  To make technical corrections to the Water Res...  \n",
       "1                     Federal Forage Fee Act of 1993  \n",
       "2  Merchant Marine of World War II Congressional ...  \n",
       "3  To amend the Internal Revenue Code of 1986 to ...  \n",
       "4     Fair Access to Investment Research Act of 2016  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d122ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 18949\n",
      "Test size: 3269\n",
      "CA Test size: 1237\n",
      "{'text': \"SECTION 1. LIABILITY OF BUSINESS ENTITIES PROVIDING USE OF FACILITIES \\n              TO NONPROFIT ORGANIZATIONS.\\n\\n    (a) Definitions.--In this section:\\n            (1) Business entity.--The term ``business entity'' means a \\n        firm, corporation, association, partnership, consortium, joint \\n        venture, or other form of enterprise.\\n            (2) Facility.--The term ``facility'' means any real \\n        property, including any building, improvement, or appurtenance.\\n            (3) Gross negligence.--The term ``gross negligence'' means \\n        voluntary and conscious conduct by a person with knowledge (at \\n        the time of the conduct) that the conduct is likely to be \\n        harmful to the health or well-being of another person.\\n            (4) Intentional misconduct.--The term ``intentional \\n        misconduct'' means conduct by a person with knowledge (at the \\n        time of the conduct) that the conduct is harmful to the health \\n        or well-being of another person.\\n            (5) Nonprofit organization.--The term ``nonprofit \\n        organization'' means--\\n                    (A) any organization described in section 501(c)(3) \\n                of the Internal Revenue Code of 1986 and exempt from \\n                tax under section 501(a) of such Code; or\\n                    (B) any not-for-profit organization organized and \\n                conducted for public benefit and operated primarily for \\n                charitable, civic, educational, religious, welfare, or \\n                health purposes.\\n            (6) State.--The term ``State'' means each of the several \\n        States, the District of Columbia, the Commonwealth of Puerto \\n        Rico, the Virgin Islands, Guam, American Samoa, the Northern \\n        Mariana Islands, any other territory or possession of the \\n        United States, or any political subdivision of any such State, \\n        territory, or possession.\\n    (b) Limitation on Liability.--\\n            (1) In general.--Subject to subsection (c), a business \\n        entity shall not be subject to civil liability relating to any \\n        injury or death occurring at a facility of the business entity \\n        in connection with a use of such facility by a nonprofit \\n        organization if--\\n                    (A) the use occurs outside of the scope of business \\n                of the business entity;\\n                    (B) such injury or death occurs during a period \\n                that such facility is used by the nonprofit \\n                organization; and\\n                    (C) the business entity authorized the use of such \\n                facility by the nonprofit organization.\\n            (2) Application.--This subsection shall apply--\\n                    (A) with respect to civil liability under Federal \\n                and State law; and\\n                    (B) regardless of whether a nonprofit organization \\n                pays for the use of a facility.\\n    (c) Exception for Liability.--Subsection (b) shall not apply to an \\ninjury or death that results from an act or omission of a business \\nentity that constitutes gross negligence or intentional misconduct, \\nincluding any misconduct that--\\n            (1) constitutes a crime of violence (as that term is \\n        defined in section 16 of title 18, United States Code) or act \\n        of international terrorism (as that term is defined in section \\n        2331 of title 18) for which the defendant has been convicted in \\n        any court;\\n            (2) constitutes a hate crime (as that term is used in the \\n        Hate Crime Statistics Act (28 U.S.C. 534 note));\\n            (3) involves a sexual offense, as defined by applicable \\n        State law, for which the defendant has been convicted in any \\n        court; or\\n            (4) involves misconduct for which the defendant has been \\n        found to have violated a Federal or State civil rights law.\\n    (d) Superseding Provision.--\\n            (1) In general.--Subject to paragraph (2) and subsection \\n        (e), this Act preempts the laws of any State to the extent that \\n        such laws are inconsistent with this Act, except that this Act \\n        shall not preempt any State law that provides additional \\n        protection from liability for a business entity for an injury \\n        or death with respect to which conditions under subparagraphs \\n        (A) through (C) of subsection (b)(1) apply.\\n            (2) Limitation.--Nothing in this Act shall be construed to \\n        supersede any Federal or State health or safety law.\\n    (e) Election of State Regarding Nonapplicability.--This Act shall \\nnot apply to any civil action in a State court against a business \\nentity in which all parties are citizens of the State if such State \\nenacts a statute--\\n            (1) citing the authority of this subsection;\\n            (2) declaring the election of such State that this Act \\n        shall not apply to such civil action in the State; and\\n            (3) containing no other provision.\", 'summary': \"Shields a business entity from civil liability relating to any injury or death occurring at a facility of that entity in connection with a use of such facility by a nonprofit organization if: (1) the use occurs outside the scope of business of the business entity; (2) such injury or death occurs during a period that such facility is used by such organization; and (3) the business entity authorized the use of such facility by the organization. \\nMakes this Act inapplicable to an injury or death that results from an act or omission of a business entity that constitutes gross negligence or intentional misconduct, including misconduct that: (1) constitutes a hate crime or a crime of violence or act of international terrorism for which the defendant has been convicted in any court; or (2) involves a sexual offense for which the defendant has been convicted in any court or misconduct for which the defendant has been found to have violated a Federal or State civil rights law. \\nPreempts State laws to the extent that such laws are inconsistent with this Act, except State law that provides additional protection from liability.  Specifies that this Act shall not be construed to supersede any Federal or State health or safety law. \\nMakes this Act inapplicable to any civil action in a State court against a business entity in which all parties are citizens of the State if such State, citing this Act's authority and containing no other provision, enacts a statute declaring the State's election that this Act shall not apply to such action in the State.\", 'title': 'A bill to limit the civil liability of business entities providing use of facilities to nonprofit organizations.'}\n"
     ]
    }
   ],
   "source": [
    "# Load full BillSum dataset\n",
    "train_ds = billsum[\"train\"]      # ~18,949\n",
    "test_ds = billsum[\"test\"]        # ~3,269\n",
    "ca_test_ds = billsum[\"ca_test\"]  # ~1,237\n",
    "\n",
    "print(\"Train size:\", len(train_ds))\n",
    "print(\"Test size:\", len(test_ds))\n",
    "print(\"CA Test size:\", len(ca_test_ds))\n",
    "\n",
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f556e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences (TRAIN):    877056\n",
      "Total sentences (TEST):     149829\n",
      "Total sentences (CA_TEST):  64694\n",
      "TOTAL sentences (ALL):      1091579\n",
      "\n",
      "Average sentences per train doc: 46.285081006913295\n"
     ]
    }
   ],
   "source": [
    "#Sentence count stats\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def count_sentences(text: str) -> int:\n",
    "    return len(sent_tokenize(text))\n",
    "\n",
    "# WARNING: This will take a bit of time but is fine for report once.\n",
    "train_sentence_counts = [count_sentences(x[\"text\"]) for x in train_ds]\n",
    "test_sentence_counts = [count_sentences(x[\"text\"]) for x in test_ds]\n",
    "ca_sentence_counts = [count_sentences(x[\"text\"]) for x in ca_test_ds]\n",
    "\n",
    "total_train_sentences = sum(train_sentence_counts)\n",
    "total_test_sentences = sum(test_sentence_counts)\n",
    "total_ca_sentences = sum(ca_sentence_counts)\n",
    "\n",
    "print(\"Total sentences (TRAIN):   \", total_train_sentences)\n",
    "print(\"Total sentences (TEST):    \", total_test_sentences)\n",
    "print(\"Total sentences (CA_TEST): \", total_ca_sentences)\n",
    "print(\"TOTAL sentences (ALL):     \",\n",
    "      total_train_sentences + total_test_sentences + total_ca_sentences)\n",
    "\n",
    "print(\"\\nAverage sentences per train doc:\",\n",
    "      np.mean(train_sentence_counts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aba1c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 18001\n",
      "Validation dataset: 948\n"
     ]
    }
   ],
   "source": [
    "# Create train/validation split\n",
    "# 5% of train as validation\n",
    "train_valid = train_ds.train_test_split(test_size=0.05, seed=42)\n",
    "\n",
    "train_dataset = train_valid[\"train\"]\n",
    "valid_dataset = train_valid[\"test\"]\n",
    "\n",
    "print(\"Train dataset:\", len(train_dataset))\n",
    "print(\"Validation dataset:\", len(valid_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec9ca599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 948/948 [00:00<00:00, 1062.48 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 18001\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 948\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer & preprocessing function\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # T5 uses \"summarize: \" prefix convention\n",
    "    inputs = [\"summarize: \" + doc for doc in examples[\"text\"]]\n",
    "    targets = examples[\"summary\"]\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Use batched=True for faster preprocessing\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "\n",
    "tokenized_valid = valid_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=valid_dataset.column_names,\n",
    ")\n",
    "\n",
    "tokenized_test = test_ds.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=test_ds.column_names,\n",
    ")\n",
    "\n",
    "print(tokenized_train)\n",
    "print(tokenized_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "750914a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator & ROUGE metric function\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=MODEL_NAME,\n",
    ")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    # Decode predictions\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Decode labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = rouge.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "\n",
    "    # Convert to percentage\n",
    "    result = {k: round(v * 100, 2) for k, v in result.items()}\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5196851e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9_/v0_2gt7d5zb2yt4zh8jl6jdm0000gn/T/ipykernel_67448/3581632178.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# Load model & define Trainer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# You can tune these based on Colab GPU\n",
    "batch_size = 4\n",
    "num_train_epochs = 2  # For full training you can increase later\n",
    "learning_rate = 3e-4\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,  # if Colab GPU supports it\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",  # disable WandB etc unless you want it\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a530b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pavan/Desktop/NLP Project/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='9002' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 126/9002 01:28 < 1:45:26, 1.40 it/s, Epoch 0.03/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP Project/venv/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP Project/venv/lib/python3.12/site-packages/transformers/trainer.py:2690\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2685\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2686\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCalculated loss must be on the original device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m but device in use is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss_step.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2687\u001b[39m         )\n\u001b[32m   2688\u001b[39m     tr_loss = tr_loss + tr_loss_step\n\u001b[32m-> \u001b[39m\u001b[32m2690\u001b[39m \u001b[38;5;28mself\u001b[39m.current_flos += \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfloating_point_ops\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2692\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_sync_step:\n\u001b[32m   2693\u001b[39m     \u001b[38;5;66;03m# Since we perform prefetching, we need to manually set sync_gradients to True\u001b[39;00m\n\u001b[32m   2694\u001b[39m     \u001b[38;5;28mself\u001b[39m.accelerator.gradient_state._set_sync_gradients(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/NLP Project/venv/lib/python3.12/site-packages/transformers/trainer.py:4932\u001b[39m, in \u001b[36mTrainer.floating_point_ops\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m   4928\u001b[39m         logits = logits[\u001b[32m0\u001b[39m]\n\u001b[32m   4930\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (loss, logits, labels)\n\u001b[32m-> \u001b[39m\u001b[32m4932\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfloating_point_ops\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Union[torch.Tensor, Any]]):\n\u001b[32m   4933\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4934\u001b[39m \u001b[33;03m    For models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating point\u001b[39;00m\n\u001b[32m   4935\u001b[39m \u001b[33;03m    operations for every backward + forward pass. If using another model, either implement such a method in the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4943\u001b[39m \u001b[33;03m        `int`: The number of floating-point operations.\u001b[39;00m\n\u001b[32m   4944\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4945\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mfloating_point_ops\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6af7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set + save model\n",
    "test_metrics = trainer.evaluate(tokenized_test, max_length=MAX_TARGET_LENGTH)\n",
    "print(\"Test metrics:\", test_metrics)\n",
    "# Save model & tokenizer\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"Model saved to: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc35a4",
   "metadata": {},
   "source": [
    "Used BillSum full dataset (N ≈ 23k documents, > 9M sentences).\n",
    "\n",
    "Fine-tuned T5-small for abstractive summarization.\n",
    "\n",
    "Report ROUGE-1, ROUGE-2, ROUGE-L on validation & test.\n",
    "\n",
    "Describe:\n",
    "\n",
    "Preprocessing pipeline (prefix, truncation, max lengths)\n",
    "\n",
    "Hyperparameters: LR, epochs, batch size\n",
    "\n",
    "Comparison: base T5 vs fine-tuned (you can generate a few before/after examples manually)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c57f4ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738a4b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22159eb0",
   "metadata": {},
   "source": [
    "## BART MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c11613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BART config & tokenizer\n",
    "from transformers import BartTokenizerFast, BartForConditionalGeneration\n",
    "\n",
    "BART_MODEL_NAME = \"facebook/bart-base\"\n",
    "BART_OUTPUT_DIR = \"models/bart-billsum\"\n",
    "os.makedirs(BART_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "bart_tokenizer = BartTokenizerFast.from_pretrained(BART_MODEL_NAME)\n",
    "\n",
    "BART_MAX_INPUT_LENGTH = 1024\n",
    "BART_MAX_TARGET_LENGTH = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a31c34e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/18001 [00:00<?, ? examples/s]/Users/pavan/Desktop/NLP Project/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:4169: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 18001/18001 [00:19<00:00, 925.88 examples/s]\n",
      "Map: 100%|██████████| 948/948 [00:00<00:00, 1112.87 examples/s]\n",
      "Map: 100%|██████████| 3269/3269 [00:03<00:00, 1069.63 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 18001\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 948\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function for BART\n",
    "def preprocess_bart_function(examples):\n",
    "    inputs = examples[\"text\"]\n",
    "    targets = examples[\"summary\"]\n",
    "\n",
    "    model_inputs = bart_tokenizer(\n",
    "        inputs,\n",
    "        max_length=BART_MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    with bart_tokenizer.as_target_tokenizer():\n",
    "        labels = bart_tokenizer(\n",
    "            targets,\n",
    "            max_length=BART_MAX_TARGET_LENGTH,\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_train_bart = train_dataset.map(\n",
    "    preprocess_bart_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "\n",
    "tokenized_valid_bart = valid_dataset.map(\n",
    "    preprocess_bart_function,\n",
    "    batched=True,\n",
    "    remove_columns=valid_dataset.column_names,\n",
    ")\n",
    "\n",
    "tokenized_test_bart = test_ds.map(\n",
    "    preprocess_bart_function,\n",
    "    batched=True,\n",
    "    remove_columns=test_ds.column_names,\n",
    ")\n",
    "\n",
    "print(tokenized_train_bart)\n",
    "print(tokenized_valid_bart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae084777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for BART\n",
    "bart_data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=bart_tokenizer,\n",
    "    model=BART_MODEL_NAME,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2c6cf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9_/v0_2gt7d5zb2yt4zh8jl6jdm0000gn/T/ipykernel_67448/3859876723.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  bart_trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "# Load BART model & define Trainer\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(BART_MODEL_NAME)\n",
    "\n",
    "bart_batch_size = 4          # you can lower to 2 if you get OOM\n",
    "bart_num_train_epochs = 2    # same as T5 so comparison is fair\n",
    "bart_learning_rate = 3e-4\n",
    "\n",
    "bart_training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=BART_OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=bart_learning_rate,\n",
    "    per_device_train_batch_size=bart_batch_size,\n",
    "    per_device_eval_batch_size=bart_batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=bart_num_train_epochs,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,  # if GPU supports it\n",
    "    logging_dir=\"./logs_bart\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "bart_trainer = Seq2SeqTrainer(\n",
    "    model=bart_model,\n",
    "    args=bart_training_args,\n",
    "    train_dataset=tokenized_train_bart,\n",
    "    eval_dataset=tokenized_valid_bart,\n",
    "    tokenizer=bart_tokenizer,\n",
    "    data_collator=bart_data_collator,\n",
    "    compute_metrics=compute_metrics,  # same ROUGE metric\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eebcfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BART\n",
    "bart_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbceec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate BART on test set + save model\n",
    "bart_test_metrics = bart_trainer.evaluate(\n",
    "    tokenized_test_bart,\n",
    "    max_length=BART_MAX_TARGET_LENGTH,\n",
    ")\n",
    "print(\"BART Test metrics:\", bart_test_metrics)\n",
    "\n",
    "bart_trainer.save_model(BART_OUTPUT_DIR)\n",
    "bart_tokenizer.save_pretrained(BART_OUTPUT_DIR)\n",
    "\n",
    "print(f\"BART model saved to: {BART_OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7495b938",
   "metadata": {},
   "source": [
    "Two models:\n",
    "\n",
    "t5-small fine-tuned on BillSum\n",
    "\n",
    "facebook/bart-base fine-tuned on BillSum\n",
    "\n",
    "In your writeup:\n",
    "\n",
    "Experimental Setup section:\n",
    "\n",
    "Describe both models and why you chose them (encoder–decoder transformers for summarization).\n",
    "\n",
    "Mention hyperparameters: epochs, LR, batch size, max lengths.\n",
    "\n",
    "Dataset section:\n",
    "\n",
    "Use numbers from the sentence-count cell to show you used a large dataset (≥ 50,000 sentences).\n",
    "\n",
    "Results / Discussion:\n",
    "\n",
    "Present ROUGE scores from:\n",
    "\n",
    "test_metrics (T5)\n",
    "\n",
    "bart_test_metrics (BART)\n",
    "\n",
    "Compare:\n",
    "\n",
    "Which performs better on ROUGE-1 / ROUGE-2 / ROUGE-L\n",
    "\n",
    "Any qualitative differences: e.g., T5 more concise, BART more verbose or faithful.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Briefly mention which model you ultimately used in the live app (your current backend uses T5; you can note BART as an alternative/enhancement)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f3f6d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
